{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1053abcb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "First, we import all required packages.\n",
    "'''\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(11)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we read in the unlabeled data from a CSV file. Each line contains one example.\n",
    "'''\n",
    "def load_data(fname = 'amazon.tsv'):\n",
    "    data = []\n",
    "    five_counter = 0\n",
    "    for count, line in enumerate(open(fname, encoding=\"utf8\", errors=\"ignore\")):\n",
    "        if count == 0:\n",
    "            continue\n",
    "        line = line.strip().split('\\t')\n",
    "        y = line[8]\n",
    "        if y == \"5\":\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "        x = line[-2]\n",
    "        data.append((x, y))\n",
    "    \n",
    "    shuffle(data)\n",
    "    \n",
    "    train_set = data[:10000]\n",
    "    dev_set = data[-1000:]\n",
    "    for element in train_set:\n",
    "        if element[1] == 1:\n",
    "            five_counter += 1\n",
    "    print(five_counter)\n",
    "    five_counter = 0\n",
    "    for element in dev_set:\n",
    "        if element[1] == 1:\n",
    "            five_counter += 1\n",
    "    print(five_counter)\n",
    "\n",
    "    return train_set, dev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Methods needed to convert the data into input that can be fed into the neural network.\n",
    "'''\n",
    "def make_feature_vectors(data, w2i):\n",
    "    new_data = []\n",
    "\n",
    "    for (x, y) in data:\n",
    "        sentence = []\n",
    "        for word in x.split(' '):\n",
    "            if word in w2i:\n",
    "                sentence.append(int(w2i[word]))\n",
    "            else:\n",
    "                sentence.append(int(w2i['<UNK>']))\n",
    "\n",
    "        new_data.append((torch.tensor(sentence, dtype=torch.long), torch.tensor([int(y)])))\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def get_vocab(data):\n",
    "    w2i = {'<UNK>': 0}\n",
    "    \n",
    "    for (x, y) in data:\n",
    "        for word in x.split(' '):\n",
    "            if word not in w2i:\n",
    "                w2i[word] = len(w2i)\n",
    "\n",
    "    return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now, we define our neural network model.\n",
    "'''\n",
    "class MyNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim):\n",
    "        super(MyNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_1 = self.embedding(input)\n",
    "        x_2 = self.lstm(x_1.unsqueeze(1))[1][0].squeeze(0).squeeze(0)\n",
    "        return self.linear(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An evaluation method so we can see how well the model is doing.\n",
    "'''\n",
    "def eval(model, data):\n",
    "    model.eval()\n",
    "    \n",
    "    total = right = 0\n",
    "    for (x, y) in data:\n",
    "        probs = F.softmax(model(x), dim=0)\n",
    "        y_hat = torch.argmax(probs)\n",
    "        if y_hat == y:\n",
    "            right += 1\n",
    "        total += 1\n",
    "    print(\"Accuracy: \" + str((right * 1.0)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030\n",
      "622\n",
      "Accuracy: 0.5105\n",
      "Accuracy: 0.531\n",
      "\n",
      "Accuracy: 0.665\n",
      "Accuracy: 0.637\n",
      "\n",
      "Accuracy: 0.7215\n",
      "Accuracy: 0.676\n",
      "\n",
      "Accuracy: 0.7713\n",
      "Accuracy: 0.671\n",
      "\n",
      "Accuracy: 0.7876\n",
      "Accuracy: 0.682\n",
      "\n",
      "Accuracy: 0.8141\n",
      "Accuracy: 0.68\n",
      "\n",
      "Accuracy: 0.8226\n",
      "Accuracy: 0.698\n",
      "\n",
      "Accuracy: 0.8442\n",
      "Accuracy: 0.704\n",
      "\n",
      "Accuracy: 0.8576\n",
      "Accuracy: 0.703\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-7ee169fd9c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mraw_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Finally, let's put everything together and train the model!\n",
    "'''\n",
    "train, dev = load_data()\n",
    "vocab = get_vocab(train)\n",
    "train = make_feature_vectors(train, vocab)\n",
    "dev = make_feature_vectors(dev, vocab)\n",
    "    \n",
    "# TODO[3]: substitute -1 by more reasonable values!\n",
    "model = MyNNModel(len(vocab), 64, 64, 2)\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.1)\n",
    "    \n",
    "eval(model, train)\n",
    "eval(model, dev)\n",
    "print()\n",
    "  \n",
    "for i in range(10):\n",
    "    model.train()\n",
    "    for (x, y) in train:\n",
    "        model.zero_grad()\n",
    "        raw_scores = model(x)\n",
    "        loss = loss_function(raw_scores.unsqueeze(0), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    eval(model, train)\n",
    "    eval(model, dev)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
